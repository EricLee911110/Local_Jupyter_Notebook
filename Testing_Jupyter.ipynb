{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca4b2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 5000\n",
    "\n",
    "train_transforms = transforms.ToTensor()\n",
    "test_transforms = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=train_transforms)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1551ec79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1= nn.Linear(784, 512)\n",
    "        self.fc2= nn.Linear(512, 512)\n",
    "        self.fc3= nn.Linear(512, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25f3cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.has_mps else 'cpu'\n",
    "certerion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7cabfe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "certerion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "226cfd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "epoch: 1 / 30..  training_loss: 0.984..  testing_loss: 0.482..  accuracy: 85.970%  time needed: 1.617\n",
      "epoch: 2 / 30..  training_loss: 0.355..  testing_loss: 0.306..  accuracy: 91.090%  time needed: 1.543\n",
      "epoch: 3 / 30..  training_loss: 0.247..  testing_loss: 0.238..  accuracy: 92.700%  time needed: 1.533\n",
      "epoch: 4 / 30..  training_loss: 0.188..  testing_loss: 0.184..  accuracy: 94.350%  time needed: 1.529\n",
      "epoch: 5 / 30..  training_loss: 0.152..  testing_loss: 0.151..  accuracy: 95.270%  time needed: 1.565\n",
      "epoch: 5 / 30..  training_loss: 0.128..  testing_loss: 0.129..  accuracy: 96.040%  time needed: 1.501\n",
      "epoch: 6 / 30..  training_loss: 0.110..  testing_loss: 0.117..  accuracy: 96.430%  time needed: 1.516\n",
      "epoch: 7 / 30..  training_loss: 0.091..  testing_loss: 0.103..  accuracy: 96.740%  time needed: 1.530\n",
      "epoch: 8 / 30..  training_loss: 0.080..  testing_loss: 0.093..  accuracy: 97.010%  time needed: 1.517\n",
      "epoch: 9 / 30..  training_loss: 0.071..  testing_loss: 0.086..  accuracy: 97.290%  time needed: 1.541\n",
      "epoch: 10 / 30..  training_loss: 0.065..  testing_loss: 0.082..  accuracy: 97.340%  time needed: 1.576\n",
      "epoch: 10 / 30..  training_loss: 0.057..  testing_loss: 0.081..  accuracy: 97.470%  time needed: 1.530\n",
      "epoch: 11 / 30..  training_loss: 0.052..  testing_loss: 0.075..  accuracy: 97.580%  time needed: 1.519\n",
      "epoch: 12 / 30..  training_loss: 0.045..  testing_loss: 0.072..  accuracy: 97.690%  time needed: 1.511\n",
      "epoch: 13 / 30..  training_loss: 0.043..  testing_loss: 0.069..  accuracy: 97.870%  time needed: 1.515\n",
      "epoch: 14 / 30..  training_loss: 0.039..  testing_loss: 0.067..  accuracy: 97.940%  time needed: 1.579\n",
      "epoch: 15 / 30..  training_loss: 0.036..  testing_loss: 0.067..  accuracy: 97.850%  time needed: 1.537\n",
      "epoch: 15 / 30..  training_loss: 0.032..  testing_loss: 0.068..  accuracy: 97.980%  time needed: 1.540\n",
      "epoch: 16 / 30..  training_loss: 0.031..  testing_loss: 0.063..  accuracy: 98.050%  time needed: 1.523\n",
      "epoch: 17 / 30..  training_loss: 0.026..  testing_loss: 0.065..  accuracy: 98.010%  time needed: 1.526\n",
      "epoch: 18 / 30..  training_loss: 0.026..  testing_loss: 0.064..  accuracy: 98.110%  time needed: 1.504\n",
      "epoch: 19 / 30..  training_loss: 0.023..  testing_loss: 0.067..  accuracy: 98.000%  time needed: 1.576\n",
      "epoch: 20 / 30..  training_loss: 0.022..  testing_loss: 0.063..  accuracy: 98.140%  time needed: 1.496\n",
      "epoch: 20 / 30..  training_loss: 0.019..  testing_loss: 0.062..  accuracy: 98.230%  time needed: 1.518\n",
      "epoch: 21 / 30..  training_loss: 0.019..  testing_loss: 0.061..  accuracy: 98.260%  time needed: 1.493\n",
      "epoch: 22 / 30..  training_loss: 0.016..  testing_loss: 0.067..  accuracy: 98.180%  time needed: 1.507\n",
      "epoch: 23 / 30..  training_loss: 0.018..  testing_loss: 0.065..  accuracy: 98.190%  time needed: 1.555\n",
      "epoch: 24 / 30..  training_loss: 0.016..  testing_loss: 0.065..  accuracy: 98.110%  time needed: 1.503\n",
      "epoch: 25 / 30..  training_loss: 0.015..  testing_loss: 0.066..  accuracy: 98.150%  time needed: 1.509\n",
      "epoch: 25 / 30..  training_loss: 0.015..  testing_loss: 0.068..  accuracy: 98.170%  time needed: 1.513\n",
      "epoch: 26 / 30..  training_loss: 0.014..  testing_loss: 0.066..  accuracy: 98.230%  time needed: 1.492\n",
      "epoch: 27 / 30..  training_loss: 0.013..  testing_loss: 0.067..  accuracy: 98.250%  time needed: 1.570\n",
      "epoch: 28 / 30..  training_loss: 0.012..  testing_loss: 0.068..  accuracy: 98.160%  time needed: 1.508\n",
      "epoch: 29 / 30..  training_loss: 0.011..  testing_loss: 0.071..  accuracy: 98.250%  time needed: 1.519\n",
      "epoch: 30 / 30..  training_loss: 0.011..  testing_loss: 0.069..  accuracy: 98.250%  time needed: 1.509\n",
      "epoch: 30 / 30..  training_loss: 0.010..  testing_loss: 0.066..  accuracy: 98.350%  time needed: 1.510\n"
     ]
    }
   ],
   "source": [
    "epoch = 30\n",
    "step = 0\n",
    "print_every = 10\n",
    "training_loss = 0\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "start = time.time()\n",
    "\n",
    "print(device)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for images, labels in trainloader:\n",
    "        step += 1 \n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        output = model(images)\n",
    "        loss = certerion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()*images.size(0)\n",
    "        \n",
    "        if (step % print_every) == 0:\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                testing_loss = 0\n",
    "                for images, labels in testloader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    output = model(images)\n",
    "                    loss = certerion(output, labels)\n",
    "                    ps = F.softmax(output, dim=1)\n",
    "                    top_score, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(top_class.shape)\n",
    "                    accuracy += torch.mean(equals.float())\n",
    "                    \n",
    "                    testing_loss += loss.item()*images.size(0)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            print(f'epoch: {e+1} / {epoch}..  '\n",
    "                  f'training_loss: {training_loss / len(trainloader.dataset):.3f}..  '\n",
    "                  f'testing_loss: {testing_loss / len(testloader.dataset):.3f}..  '\n",
    "                  f'accuracy: {accuracy /len(testloader) * 100:.3f}%  '\n",
    "                  f'time needed: {(time.time() - start):.3f}')\n",
    "            \n",
    "            start = time.time()\n",
    "            training_loss = 0\n",
    "            model.train()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e88cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True],\n",
      "        [ True, False],\n",
      "        [ True, False]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],\n",
    "                  [3,4],\n",
    "                  [5,6]])\n",
    "b = torch.tensor([[1,2],\n",
    "                  [3,3],\n",
    "                  [5,7]])\n",
    "\n",
    "c = a == b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b4623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1678922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: tensun\n",
      "Your Kaggle Key: ········\n",
      "Downloading dogs-vs-cats.zip to ./dogs-vs-cats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 812M/812M [28:05<00:00, 505kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./dogs-vs-cats/dogs-vs-cats.zip to ./dogs-vs-cats\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Cat_Dog_data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m test_transforms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     19\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m255\u001b[39m),\n\u001b[1;32m     20\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     21\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     22\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     23\u001b[0m ])\n\u001b[1;32m     26\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCat_Dog_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m trainlader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m test_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/test\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtest_transforms)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torchvision/datasets/folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    304\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m ):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torchvision/datasets/folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 145\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torchvision/datasets/folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Cat_Dog_data/train'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim\n",
    "from collections import OrderedDict\n",
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/competitions/dogs-vs-cats/data\")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "data_dir = 'Cat_Dog_data'\n",
    "\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "trainlader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a79abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(1024, 500)),\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('fc2', nn.Linear(500, 2)),\n",
    "    ('output', nn.LogSoftmax(dim=1))\n",
    "]))\n",
    "\n",
    "model.classifier = classifier\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "certerion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
    "\n",
    "epoch = 1\n",
    "print_every = 5\n",
    "step = 0\n",
    "\n",
    "for ii, (inputs, labels) in enumerate():\n",
    "\n",
    "    inputs, labels = inpute.to(device), labels.to(device)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(inputs)\n",
    "    loss = certerion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if ii == 3:\n",
    "        break\n",
    "\n",
    "    print(f'Device: {device}; Time per batch: {(time.time() - start)/3:.3f} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ed57ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722469b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.. eric\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'hello.. '\n",
    "    f'eric'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4dcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869aa7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23741eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "    this is the first line of code inlind with text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a118372",
   "metadata": {},
   "source": [
    "    this is a line of code inline with text\n",
    "LOL what was that singing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "````\n",
    "print('hello')\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbecf04f",
   "metadata": {},
   "source": [
    "markdowwwwn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4263d",
   "metadata": {},
   "source": [
    "````\n",
    "code block\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db604e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,0,1])[:,None]\n",
    "c = 3\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "a = \"The Dog jumps over the roof\".lower()\n",
    "b = \"The cat jumps on the roof\".lower()\n",
    "\n",
    "vocab = Counter()\n",
    "for word in a.split(' '):\n",
    "    vocab[word] += 1\n",
    "    \n",
    "print(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "for i,word in enumerate(vocab):\n",
    "    print(i,vocab[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "vocab.add(\"ehllo\")\n",
    "vocab.add('ehllo')\n",
    "vocab.add(\"Hello\")\n",
    "vocab.add(\"hello\")\n",
    "print(vocab)\n",
    "for i,word in enumerate(vocab):\n",
    "    print(i, word)\n",
    "vocab_list = list(vocab)\n",
    "print(vocab_list)\n",
    "for i,word in enumerate(vocab_list):\n",
    "    print(i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc24d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights = np.zeros([4,1])\n",
    "print(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b58427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student:\n",
    "    def __init__(self,age,name,update_age):\n",
    "        self.age = age\n",
    "        self.name = name\n",
    "        self.update_age(update_age)\n",
    "    def update_age(self, age):\n",
    "        self.age = age\n",
    "        print('Done! Your age is: ', self.age)\n",
    "student1 = Student(18, 'eric', 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cedf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(2==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3]])\n",
    "b = np.array([[4],\n",
    "              [5],\n",
    "              [6]])\n",
    "c = 10\n",
    "print(b.dot(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(2,2)\n",
    "b = np.zeros((2,2))\n",
    "print(a.dot(10))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Cute dog\"\n",
    "if (\"dog\" in a.split(' ')):\n",
    "    print(\"something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0af21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros((1,10))\n",
    "print(zeros)\n",
    "print(zeros.size)\n",
    "a = np.array([1,2,3,4])\n",
    "print(a)\n",
    "print(a.size)\n",
    "\n",
    "zeros[8] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40444fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5,6,7,8,9])\n",
    "print(a[:-2])\n",
    "print(a[-2:])\n",
    "print('POSITIVE' == \"POSITIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1]\n",
    "a[9] = 10\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"german shepherd is the cutest\"\n",
    "for i,word in enumerate(a.split(' ')):\n",
    "    print(i,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(3,3)\n",
    "print(a)\n",
    "print(a[1][:])\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b70c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.zeros((1, 2))\n",
    "print(indices)\n",
    "indices[0][0] = 4\n",
    "indices[0][1] = 10\n",
    "print(indices)\n",
    "\n",
    "for index in indices[0]:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"I like dogs and cats\"\n",
    "for i,word in enumerate(a.split(' ')):\n",
    "    print(i, word)\n",
    "    print(type(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set([1,2,3])\n",
    "print(a)\n",
    "b = list(a)\n",
    "print(b)\n",
    "print(type(b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "a = Counter()\n",
    "review = \"German shepherd is the cutest dog dog :D\"\n",
    "for word in review.split(' '):\n",
    "    a[word] += 1\n",
    "print(a)\n",
    "\n",
    "word_count2 = Counter()\n",
    "\n",
    "for word,cnt in list(a.most_common()):\n",
    "    print(word)\n",
    "    print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "b = np.array(2)\n",
    "print(a.dot(b))\n",
    "print(b.dot(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee66ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1<2 and 2>1:\n",
    "    print(\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fde41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"dog cat\"\n",
    "if \"dog\" in a and \n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24120494",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range 5:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6])\n",
    "y = [1,2,3,4,5,6]\n",
    "hidden_error_term = [[1,2],\n",
    "                     [3,4]]\n",
    "#print(hidden_error_term *x[:, None])\n",
    "\n",
    "print(np.dot([2], x))\n",
    "print(x*[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c03d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2])\n",
    "y = np.array([[1],\n",
    "     [2]])\n",
    "c = np.array([2,3])\n",
    "\n",
    "output = np.dot(x, y)\n",
    "print(output)\n",
    "print(output - 2)\n",
    "\n",
    "hidden_error = np.dot(y, output)\n",
    "hidden_error_term = hidden_error *c\n",
    "\n",
    "print(hidden_error)\n",
    "print(1- hidden_error_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1])\n",
    "y = np.array([3,4])\n",
    "\n",
    "print(np.dot(x, [y]))\n",
    "print(y.T)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn((2,3))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # pip install datasets\n",
    "\n",
    "# load the first 1K rows of the TREC dataset\n",
    "trec = load_dataset('trec', split='train[:1000]')\n",
    "trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26485134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel  # pip install transformers\n",
    "\n",
    "# initialize BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# take the first 64 rows of the trec data\n",
    "text = trec['text'][:64]\n",
    "# tokenize text using the BERT tokenizer\n",
    "tokens = tokenizer(\n",
    "    text, max_length=512,\n",
    "    truncation=True, padding=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80da55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "tokens.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da810cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
