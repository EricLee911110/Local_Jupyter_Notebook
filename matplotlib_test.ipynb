{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 300)\n",
    "for w in range(2, 6, 2):\n",
    "    plt.plot(x, np.sin(np.pi*x)*np.sin(2*w*np.pi*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5491a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s = np.array(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09210d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.shape)\n",
    "print(s+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[1,2,3,4,5,6,7,8,9,10,11]\n",
    "a = np.array(inputs)[None, :]\n",
    "print(a)\n",
    "\n",
    "min_value = np.amin(a)\n",
    "inputs_minus_min = a - min_value\n",
    "print(inputs_minus_min)\n",
    "\n",
    "max_value = np.amax(inputs_minus_min)\n",
    "inputs_div_max = inputs_minus_min / max_value\n",
    "print(inputs_div_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076593a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = [[1,2,3],\n",
    "      [4,5,6],\n",
    "      [7,8,9]]\n",
    "\n",
    "m2 = [[1,2],\n",
    "      [3,4],\n",
    "      [5,6]]\n",
    "\n",
    "result = np.dot(m1,m2)\n",
    "print(result)\n",
    "\n",
    "print(np.shape(m1))\n",
    "print(np.shape(m2)[1])\n",
    "print(np.sum(m1))\n",
    "\n",
    "print(np.count_nonzero(m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44729482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "v1 = np.array([1,2])\n",
    "v2 = np.array([3,4])\n",
    "\n",
    "A = int(0.9 > 1.2)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24467ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -1.1\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b858bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    # Fill in code\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i], W, b)\n",
    "        if y[i] - y_hat == 1:  #Negatively misclassified\n",
    "            W, b = W + X[i].reshape(2,1)*learn_rate, b + 1*learn_rate\n",
    "        elif y[i] - y_hat == -1:  #Positively misclassifies \n",
    "            W, b = W - X[i].reshape(2,1)*learn_rate, b - 1*learn_rate\n",
    "    \n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41cb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#2-D Situation\n",
    "x1 = np.array([[1,2],\n",
    "               [3,4]]) \n",
    "x2 = np.array([[11,12],\n",
    "               [13,14]])\n",
    "print(np.dot(x1,x2))\n",
    "print(np.matmul(x1,x2))\n",
    "\n",
    "print('_____________')\n",
    "print(np.multiply(x1,x2))\n",
    "print(x1*x2)\n",
    "\n",
    "print('____________')\n",
    "print(np.inner(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e38a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.array([1,2])\n",
    "x2 = np.array([3,4])\n",
    "\n",
    "print(np.dot(x1,x2))\n",
    "print(np.matmul(x1,x2))\n",
    "print(np.inner(x1,x2))\n",
    "result = 1 if np.matmul(x1,x2) > 0 else 0\n",
    "print(result)\n",
    "\n",
    "print(np.multiply(x1,x2))\n",
    "print(x1*x2)\n",
    "\n",
    "print('__________________')\n",
    "print(x1+x2*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1765206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.array([1,2])\n",
    "x2 = np.random.rand(2,1)\n",
    "\n",
    "print((np.dot(x1,x2) + 1)[0])\n",
    "\n",
    "print(x1)\n",
    "x1 = x1.reshape(2,1)\n",
    "print(x1)\n",
    "x2 = np.add(x1,x2)\n",
    "print(x2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d416e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "x = np.float_(a)\n",
    "print(x*(b+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8668c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [1,2,3]\n",
    "print(x1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('cd0281-Introduction-to-Neural-Networks-with-PyTorch-master/gradient-descent/data.csv', header=None)\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(data[[0,1]])\n",
    "print(data[2])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe501b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "print('weights:', weights)\n",
    "\n",
    "x1 = [1,2]\n",
    "\n",
    "print(np.matmul(weights, x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a217ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_features = 100\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53f951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b39607b",
   "metadata": {},
   "source": [
    "## Gradient Descent of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Activation of the output unit\n",
    "        #   Notice we multiply the inputs and the weights here \n",
    "        #   rather than storing h as a separate variable \n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # The error, the target minus the network output\n",
    "        error = y - output\n",
    "\n",
    "        # The error term\n",
    "        #   Notice we calulate f'(h) here instead of defining a separate\n",
    "        #   sigmoid_prime function. This just makes it faster because we\n",
    "        #   can re-use the result of the sigmoid function stored in\n",
    "        #   the output variable\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "        # The gradient descent step, the error times the gradient times the inputs\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # Update the weights here. The learning rate times the \n",
    "    # change in weights, divided by the number of records to average\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545be3fa",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf698f1",
   "metadata": {},
   "source": [
    "4x3x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b16312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d60cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd1449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
